About deep Learning - 
It is a subset of machine learning that utilizes artificial neural networks with multiple layers (hence the term "deep") to learn and extract patterns from data. 
These neural networks are inspired by the structure and function of the human brain, consisting of interconnected nodes called neurons organized into layers. 
Each layer of neurons processes specific features of the input data and passes them on to the next layer for further processing.
Deep learning algorithms can automatically learn representations of data through the iterative process of forward propagation and backpropagation. 
During forward propagation, the input data is fed through the network, and the output is compared to the actual target values to compute the error. 
Then, during backpropagation, this error is propagated backward through the network, and the model's parameters (weights and biases) are adjusted to minimize the error, typically using optimization techniques like gradient descent.
It includes computer vision, natural language processing, speech recognition, and reinforcement learning. Some popular deep learning architectures include convolutional neural networks (CNNs) for image processing, recurrent neural networks (RNNs) for sequential data, and transformers for natural language processing tasks. Deep learning has significantly advanced the state-of-the-art in many artificial intelligence applications and continues to be an active area of research and development.

About Neocortex:
Neocortex is a mammalian development and is almost like a dinner napkin squeezed in our skull. 
In general, whenever we talk about “brain” or “intelligence” in colloquial terms, we are almost always referring to the Neocortex. 
An interesting fact about Neocortex is that the cellular structure throughout all these regions is almost the same, whether it be from the visual processing region or the audio processing region. 
This finding is extremely important as this means that the brain is trying to solve similar problems to process any kind of sensory data – visual, audio etc. 
These regions are logically related to each other in a hierarchical structure.
The sensory data is represented as simple ideas in the lower level and the idea gets more abstract in the higher level. 
A parallel to this process in the deep learning space – the initial layers in neural networks detect simple ideas like edges, intermediate layers detect shapes, and final layers identify objects.
Hebbian learning is one of the oldest learning algorithms and works on an extremely simple principle – synapse between two neurons is strengthened when the neurons on either side of the synapse (input and output) have highly correlated outputs.

Commercially Tested Use Cases:-
1. Sever Monitoring: 
Grok was the first commercially available product from Numenta that is used for anomaly detection. 
One of the important use cases is anomalies in server monitoring. Servers generate a lot of metrics like CPU usage, GPU usage, etc. at a high frequency. 
Grok creates an SDR for individual metrics and feeds it into individual HTM models. 
The HTM model then sends a trigger to the user whenever it sees an anomaly for any of the metrics.

2. Stock volume anomalies:
This application tracks the stock price and Twitter data of publicly traded companies and sends an alert whenever anomalies are detected to take timely buy/sell action.

3. Rouge Human behaviour:
Rogue human behavior is an application based on HTM that can help automate discovering security issues.Like, identifying abnormal financial trading activities or asset allocations by individual traders, 
sending alerts when employee behaviors or actions fall outside of typical patterns, detect the installation, activation, or usage of unapproved software,
sending alerts when employee computers or devices are used by unauthorized individuals.

4. Natural language prediction:
It refers to the task of using computational methods to anticipate or forecast aspects of human language. 
It involves building models that can understand and generate natural language text based on input data. 
These models aim to predict various linguistic properties or structures, such as the next word in a sentence, sentiment of a text, part-of-speech tags, semantic relationships,

5. Geospatial Tracking:
Geospatial tracking takes the input of various locations (latitude or longitude) and the velocity of a vehicle or even humans. 
We convert these metrics into SDR and feed them into the HTM model. The model will send you a trigger if it sees any anomaly in the data. 
This tracking can be used by any transportation or cab service to track all their vehicles. 

Working of HTM:
Input temporal data generated from various data sources is semantically encoded as a  sparse array called as sparse distributed representation (SDR).  
This encoded array goes through a processing called spatial pooling to normalize/standardize the  input data from various sources into a sparse output vector or mini-columns (column of pyramidal neurons) of definitive size and fixed sparsity. 
The learning of this spatial pooling is done through Hebbian learning with the boosting of prolonged inactive cells. 
The spatial pooling retains the context of the input data by an algorithm called temporal memory.
initially tries to learn a route that is shown in all red. Gradually it learns each latitude, longitude and velocity with a particular sequence. 
Any change in route or speed or direction will be tracked as an anomaly and reported back.

A: SDR - Sparse Distributed Representation
SDR is simply an array of 0’s and 1’s. If you take a snapshot of neurons in the brain, likely, you will only see less than 2% neurons in an active state. 
SDR is a mathematical representation of these sparse signals which will likely have less than 2% ones. 
Properties of SDR:
a. SDR is extremely noise resistant. If you introduce noise in magnitude of say 33%, highly likely that the SDR will only match with very like objects. 
You will realize this even for human memory. Try to remember a  name of a professor from your graduation college and try remembering his/her face. It is possible that you might not remember him/her face clearly. 
Even then, if I show you 100 random picture which include your professor’s image, you will definitely locate your professor with high accuracy.
b. SDR can be sub-sampled without loosing much information. Say, your SDR has 20 ones and 10000 zeros. Even if you remove 5 zeros from this SDR, we can still use the sub-sampled SDR to match with new SDRs. You will realize this even for human memory. 
Even if we forget a lot of properties of a fruit which we tasted 10 years back, we can still classify this fruit accurately based on all other properties that we do remember.

B: Semantic Encoding -
We use an encoding engine to take input from an input source and create an SDR. We need to make sure that the encoding algorithm gives us similar SDR for similar objects. 
This concept is very similar to embedding in the deep learning space. 
A lot of pre-built encoders are already available online that include numeric encoding, datetime encoding, English word encoding, 
We need to create your own encoder for specific problems. 
Step 1 – We choose a set of documents that we will use to find semantics of words
Step 2 – We will clean the documents and slice each document into snippets. We will then cluster these snippets so that similar snippets are kept together
Step 3 –  We will now represent each snippet as a node in a SDR
Step 4 – Now we pick up individual (target) words and activate all the nodes (documents) in the SDR that contain our target word. This will create a word fingerprint in form of SDR
Step 5 – We will repeat the above four steps to get word fingerprints for all the words we are interested.

C.SPATIAL POOLING -
Spatial pooling is the process of converting the encoded SDR into a sparse array complying with two basic principles:
a. Make sure the sparsity of the output array is constant at all times, even if the input array is a very sparse SDR or a not so sparse SDR
b. Make sure the overlap or semantic nature of the input is maintained
So the overlap of both input and output SDR of two similar objects need to be high. 
The input vector had a sparsity varying from 33% to 67%, but the spatial pooling made sure the sparsity of the output array is 33%. 
Also the semantics of the two possible inputs in the series are completely different from each other, and the same was maintained in the output vector.

D. Hebbian learning-
Learning in HTM is based on a very simple principle. 
The synapse between the active column in the spatially pooled output array, and active cells in encoded sequence, is strengthened. 
The synapses between the active column in the spatially pooled output array, and inactive cells in encoded input, is weakened. 
This process is repeated again and again to learn patterns.

E. Temporal Memory-
Spatial pooling maintains the context of the input sequence by a method called temporal memory. 
The concept of temporal memory is based on the fact that each neuron not only gets information from lower level neurons, but also gets contextual information from neurons at the same level.  
In the spatial pooling section, each column in the output vector by a single number. However, each column in the output column is comprised of multiple cells that can individually be in active, inactive, or predictive state.

Step 1 : 
HTM model gets an input “1” for the first time which activates the first column of the output sequence. 
Because none of the cells in the first column were in predictive mode, we say column 1 goes “burst” and we assign an active value to each of the cells in column 1. 
Step 2 :  
HTM model gets an input “2” again for the first time in the context of “1”, and hence, none of its cells are in predictive state so column 2 goes burst.
Same thing happens at step 3.
Step 4 - 
Our HTM model has seen “2” in context of “1” before, so it tries to make a prediction
The method it uses to make this prediction is as follows: It checks with all the cells that are currently active, i.e., column 1, to tell which of the 9 cells do they predict will turn active in the next time step. 
Say, the synapse between (2,2) cell is stronger with column 1 among (2,1),(2,2) and (2,3), so column 1 unanimously replies (2,2). 
Now (2,2) is put into a predictive state before consuming our next element of the sequence. 
Once our next element arrives, which is actually a “2”, the prediction goes right and none of the columns burst this time.
Step 5 - none of the columns burst and only (1,1) is put in active state as (1,1) had a strong synapse with (2,2).
Step 6 -  HTM model is expecting a value of “2” but it gets “1”. 
Hence, our first column goes burst and our anomaly is detected in this sequence.

Structure of HTM-
In the human neocortex, neurons have thousands of excitatory synapses, clustered onto a set of dendrites:
These synapses can fire together in different patterns to cause a spike (high action potential) in the cell,
A- Proximal dendrites — 
It receives feedforward input from the input space and other cells at lower levels. These synapses have a large effect on the action potential of the neuron and hence define the “receptive field” of the neuron.
Co-activation of synapses contribute to the action potential of the neuron and if the inputs to the synapses are sparsely active, the neuron can recognize multiple unique feedforward patterns.
Basal synapses — 
These receive contextual inputs from nearby cells in the same region and recognize patterns that precede the firing of a cell. 
These synapses depolarize the cell, contributing to the action potential but are not large enough to generate an action potential as proximal synapses do. These act as a prediction mechanism for the next feedforward input.
Apical synapses — These provide top-down feedback (from cells in higher layers) and depolarize a cell similar to a basal synapse (these cannot generate an action potential either). 
These can predict multiple elements at a time and act as a prediction mechanism for the entire sequence.

Hence, feedforward inputs (proximal dendrites) activate cells, while basal and apical inputs (basal synapses) generate predictions.

HTM Learning-
In HTMs, learning occurs by changing the permanence values of its synapses (inputs to the cell) which in turn affects the weight of that connection. 
This is a bit similar to how a point neuron in an artificial neural network updates its weight matrix on its incoming connections, however, it does so at a neuron level (due to the simplified dendritic mode). 
The HTM model has 3 dendritic zones
--Proximal — feedforward input weights
--Basal —contextual predictive weights
--Apical — top-down predictive weights and uses Hebbian learning to update synapses at each dendrite (at a synaptic level) and not the entire neuron.

Working on existing Temporal memory test cases:
I created a new class to run my test cases on temporal memory

1. public TestContext TestContext
This method give context based on information provided about the expected active cell and actual active cell. 

int[] activeColumns = { 0, 1, 2, 3 };
Cell[] activeCells = cn.GetCells(new int[] { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 });
Here we activate 4 columns in input space. A set of cells from 0-9 is called from GetCells method of Connections class. 
This is stored as "active cells" in class object Cell. The cell is one neuron that is taking these inputs to run its class fully.

ComputeCycle cycle = tm.Compute(activeColumns, true) as ComputeCycle;
-ComputeCycle hold all important states calculated in a cycle
Full calculation of Temporal memory(TM) happens here. It calculates 2 things: 
1. Cells that get active in current cycle
2. Dendrites that get active in current cycle
true is a bool learn means whether the cycle should include learning or not.
TM class uses the interface IHTMAlgorithm to input integer arrays and output of type Compute

get { return TestContextInstance; } - It gets the input values from above methods
set { TestContextInstance = value; }- Those input value are store in variable value

2. private static bool areDisjoined<T>(ICollection<T> arr1, ICollection<T> arr2)
{
    foreach (var item in arr1)
    {
        if (arr2.Contains(item))
            return false;
    }

    return true;

This code checks if two arrays of numbers are disjointed(means they have no common elements).
Array 2 is taken from Collection interface. Contains(item) is a bool, is a method inside this interface which checks  Each element of arr1 with each element in arr2. 
If yes, then it return false, else true. 
ICollection uses IEnumerable that has GetEnumerable method which is used to perform all kinds of operations on arrays.

3. Current configuration of temporal memory is-
All temporal memory parameters are defined under KEY class
a. 5 cells per column
b. If active connected synapses reaches activation threshold 3, then that segment is said to be active segment.
c. Permanance value must start with 0.21
d. If a permanance value of a synapse is more than 0.5, then synapse is said to be connected.
e. If number of synapses active on a segment is 2, it is called as best matching cell in that column
f. The maximum number of segments that can be added to a segment while learning is 3
g. During learning, the amount of permanance value that should be incremented is 0.10
h. During learning, the amount of permanance value that should be decremented is 0.10
i. During learning, the amount of permanance value of synapses that was active previously  should be decremented by 0.

Defaults_temporal is a dictionary that stores temporal values with string keys and object values.
retVal is a variable that stores this dictionary.

retVal.Set(KEY.RANDOM, new ThreadSafeRandom(42));
This line calls retVal dictionary. It sets random values inside dictionary. Thread-safe random means the random numbers generated will be accessed by one thread each time. 42 is a number to initialise the random number generator.

 private Parameters GetDefaultParameters(Parameters p, string key, Object value)
 {
     Parameters retVal = p == null ? GetDefaultParameters() : p;
     retVal.Set(key, value);

     return retVal;
 }
Parameters retVal = p == null ? GetDefaultParameters() : p;: This line initializes a local variable retVal of type Parameters. It checks if the parameter p is null. If it is null, it calls GetDefaultParameters() method to get default parameters. If p is not null, it assigns p to retVal.
retVal.Set(key, value);: This line calls the Set method on the retVal object, passing in the key and value parameters. This method presumably sets the given key-value pair in the Parameters object.
return retVal;: Finally, the method returns the retVal, which contains the updated parameters. this method is designed to either update existing parameters or create a new Parameters object with the default values if the input parameter p is null. It then sets a key-value pair in the parameters and returns the updated or newly created Parameters object.

Next, we retrieve default configurations

Test1: 
public void TestNewSegmentGrowthWhenMultipleMatchingSegmentsFound()
{
    // Initialization
    TemporalMemory tm = new TemporalMemory();
    Connections cn = new Connections();
    Parameters p = Parameters.getAllDefaultParameters();
    p.apply(cn);
    tm.Init(cn);

    // Define active columns and corresponding active cells
    int[] activeColumns = { 0 };
    Cell[] activeCells = { cn.GetCell(0), cn.GetCell(1), cn.GetCell(2), cn.GetCell(3) };

    // Create multiple matching segments for the active cell
    DistalDendrite dd1 = cn.CreateDistalSegment(activeCells[0]);
    cn.CreateSynapse(dd1, cn.GetCell(4), 0.3);
    cn.CreateSynapse(dd1, cn.GetCell(5), 0.3);

    DistalDendrite dd2 = cn.CreateDistalSegment(activeCells[0]);
    cn.CreateSynapse(dd2, cn.GetCell(6), 0.3);
    cn.CreateSynapse(dd2, cn.GetCell(7), 0.3);

    // Execute computation cycle
    tm.Compute(activeColumns, true);

    // Verify that a new segment has been grown
    Assert.AreEqual(2, activeCells[0].DistalDendrites.Count);

    DistalDendrite newSegment = activeCells[0].DistalDendrites[0] as DistalDendrite;

    // Additional assertions for the new segment
    Assert.IsNotNull(newSegment);
    Assert.AreEqual(2, newSegment.Synapses.Count);
}
This unit test seems to be testing the functionality related to growing new segments in a temporal memory model when multiple matching segments are found for an active cell. 
It sets up the scenario where two distal segments are associated with an active cell and checks whether the computation cycle grows a new segment correctly and whether this new segment meets certain criteria. 
The Assert statements ensure that the behavior of the temporal memory class conforms to the expected behavior described in the unit test.

Test 2:
public void TestSynapsePermanenceUpdateWhenMatchingSegmentsFound()
{
    // Create instances of TemporalMemory, Connections, and Parameters objects.
    TemporalMemory tm = new TemporalMemory();
    Connections cn = new Connections();
    Parameters p = GetDefaultParameters(null, KEY.PERMANENCE_DECREMENT, 0.08); // Uses Permanence decrement parameter 
    p.apply(cn);
    tm.Init(cn);

    // Define previous and current active columns and cells.
    int[] previousActiveColumns = { 0 };
    int[] activeColumns = { 1 };
    Cell[] previousActiveCells = { cn.GetCell(0), cn.GetCell(1), cn.GetCell(2), cn.GetCell(3) };
    Cell[] activeCells = { cn.GetCell(4), cn.GetCell(5) };

    // Create a matching segment for the first active cell.
    DistalDendrite selectedMatchingSegment = cn.CreateDistalSegment(activeCells[0]);
    cn.CreateSynapse(selectedMatchingSegment, previousActiveCells[0], 0.3);
    cn.CreateSynapse(selectedMatchingSegment, previousActiveCells[1], 0.3);
    cn.CreateSynapse(selectedMatchingSegment, previousActiveCells[2], 0.3);
    cn.CreateSynapse(selectedMatchingSegment, cn.GetCell(81), 0.3);

    // Create another matching segment for the second active cell.
    DistalDendrite otherMatchingSegment = cn.CreateDistalSegment(activeCells[1]);
    Synapse as1 = cn.CreateSynapse(otherMatchingSegment, previousActiveCells[0], 0.3);
    Synapse is1 = cn.CreateSynapse(otherMatchingSegment, cn.GetCell(81), 0.3);

    // Perform two cycles of activity.
    tm.Compute(previousActiveColumns, true);
    tm.Compute(activeColumns, true);

    // Assert that the synapse permanence of matching synapses has been updated.
    Assert.AreEqual(0.3, as1.Permanence, 0.01);
    Assert.AreEqual(0.3, is1.Permanence, 0.01);
}
This test ensures that when the temporal memory undergoes activity cycles, the synapse permanence of matching synapses within matching segments is updated as expected. 
Asserting Synapse Permanence Update: It asserts that the synapse permanence of matching synapses has been updated correctly after the computation cycles. 
It checks whether the permanence of the synapses as1 and is1 is equal to 0.3 with a tolerance of 0.01.

public void TestCellsPerColumn()
{
    // Arrange
    TemporalMemory tm = new TemporalMemory();
    Connections cn = new Connections();
    Parameters p = Parameters.getAllDefaultParameters();
    p.Set(KEY.COLUMN_DIMENSIONS, new int[] { 64, 64 });
    p.Set(KEY.CELLS_PER_COLUMN, 16); // Set a custom number of cells per column
    p.apply(cn);
    tm.Init(cn);

    // Act
    int totalCellCount = 0;
    foreach (var column in cn.GetColumns())
    {
        totalCellCount += column.Cells.Length;
    }

    // Assert
    Assert.AreEqual(64 * 64 * 16, totalCellCount);
}
Analysis - The purpose of this test is to ensure that the temporal memory is correctly initialized with the specified parameters, and that the total number of cells in the model matches the expected value based on these parameters. The Assert statement verifies that the actual total number of cells matches the expected number. 
If it does, it confirms that the temporal memory has been initialized correctly according to the specified parameters.

public void TestCustomDimensionsAndCells()
{
    // Arrange
    TemporalMemory tm = new TemporalMemory();
    Connections cn = new Connections();
    Parameters p = Parameters.getAllDefaultParameters();
    p.Set(KEY.COLUMN_DIMENSIONS, new int[] { 16, 32 }); // Set custom column dimensions
    p.Set(KEY.CELLS_PER_COLUMN, 8); // Set custom number of cells per column
    p.apply(cn);

    // Act
    tm.Init(cn);

    // Calculate the expected total number of cells based on custom dimensions
    int expectedTotalCells = 16 * 32 * 8;

    // Sum the actual number of cells in all columns
    int actualTotalCells = cn.GetColumns().Sum(column => column.Cells.Length);

    // Assert
    Assert.AreEqual(expectedTotalCells, actualTotalCells);
}
The test verifies that the temporal memory initializes correctly with the specified custom parameters and that the total number of cells in the model matches the expected value based on these parameters. 
If the assertion passes, it confirms that the temporal memory has been initialized as expected with the custom dimensions and cells.

public void TestColumnDimensions()
{
    // Arrange
    // Initialize a TemporalMemory object, Connections object, and set custom column dimensions in the parameters.
    TemporalMemory tm = new TemporalMemory();
    Connections cn = new Connections();
    Parameters p = Parameters.getAllDefaultParameters();
    p.Set(KEY.COLUMN_DIMENSIONS, new int[] { 32, 64 }); // Set custom column dimensions
    p.Set(KEY.CELLS_PER_COLUMN, 32);
    p.apply(cn);
    tm.Init(cn);

    // Act
    // Count the total number of cells in all columns and verify it against the expected count.
    int totalCellCount = 0;
    foreach (var column in cn.GetColumns())
    {
        totalCellCount += column.Cells.Length;
    }

    // Assert
    // Check if the total cell count matches the expected count based on custom column dimensions.
    Assert.AreEqual(32 * 64 * 32, totalCellCount);
}
It ensures that when custom column dimensions are set, the TemporalMemory instance initializes correctly with the specified number of cells per column, and the total number of cells in the model matches the expected count based on these custom dimensions. 
If the assertion passes, it confirms that the TemporalMemory has been initialized as expected with the custom column dimensions.
