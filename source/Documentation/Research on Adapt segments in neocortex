About deep Learning - 
It is a subset of machine learning that utilizes artificial neural networks with multiple layers (hence the term "deep") to learn and extract patterns from data. 
These neural networks are inspired by the structure and function of the human brain, consisting of interconnected nodes called neurons organized into layers. 
Each layer of neurons processes specific features of the input data and passes them on to the next layer for further processing.
Deep learning algorithms can automatically learn representations of data through the iterative process of forward propagation and backpropagation. 
During forward propagation, the input data is fed through the network, and the output is compared to the actual target values to compute the error. 
Then, during backpropagation, this error is propagated backward through the network, and the model's parameters (weights and biases) are adjusted to minimize the error, typically using optimization techniques like gradient descent.
It includes computer vision, natural language processing, speech recognition, and reinforcement learning. Some popular deep learning architectures include convolutional neural networks (CNNs) for image processing, recurrent neural networks (RNNs) for sequential data, and transformers for natural language processing tasks. Deep learning has significantly advanced the state-of-the-art in many artificial intelligence applications and continues to be an active area of research and development.

About Neocortex:
Neocortex is a mammalian development and is almost like a dinner napkin squeezed in our skull. 
In general, whenever we talk about “brain” or “intelligence” in colloquial terms, we are almost always referring to the Neocortex. 
An interesting fact about Neocortex is that the cellular structure throughout all these regions is almost the same, whether it be from the visual processing region or the audio processing region. 
This finding is extremely important as this means that the brain is trying to solve similar problems to process any kind of sensory data – visual, audio etc. 
These regions are logically related to each other in a hierarchical structure.
The sensory data is represented as simple ideas in the lower level and the idea gets more abstract in the higher level. 
A parallel to this process in the deep learning space – the initial layers in neural networks detect simple ideas like edges, intermediate layers detect shapes, and final layers identify objects.
Hebbian learning is one of the oldest learning algorithms and works on an extremely simple principle – synapse between two neurons is strengthened when the neurons on either side of the synapse (input and output) have highly correlated outputs.

Commercially Tested Use Cases:-
1. Sever Monitoring: 
Grok was the first commercially available product from Numenta that is used for anomaly detection. 
One of the important use cases is anomalies in server monitoring. Servers generate a lot of metrics like CPU usage, GPU usage, etc. at a high frequency. 
Grok creates an SDR for individual metrics and feeds it into individual HTM models. 
The HTM model then sends a trigger to the user whenever it sees an anomaly for any of the metrics.

2. Stock volume anomalies:
This application tracks the stock price and Twitter data of publicly traded companies and sends an alert whenever anomalies are detected to take timely buy/sell action.

3. Rouge Human behaviour:
Rogue human behavior is an application based on HTM that can help automate discovering security issues.Like, identifying abnormal financial trading activities or asset allocations by individual traders, 
sending alerts when employee behaviors or actions fall outside of typical patterns, detect the installation, activation, or usage of unapproved software,
sending alerts when employee computers or devices are used by unauthorized individuals.

4. Natural language prediction:
It refers to the task of using computational methods to anticipate or forecast aspects of human language. 
It involves building models that can understand and generate natural language text based on input data. 
These models aim to predict various linguistic properties or structures, such as the next word in a sentence, sentiment of a text, part-of-speech tags, semantic relationships,

5. Geospatial Tracking:
Geospatial tracking takes the input of various locations (latitude or longitude) and the velocity of a vehicle or even humans. 
We convert these metrics into SDR and feed them into the HTM model. The model will send you a trigger if it sees any anomaly in the data. 
This tracking can be used by any transportation or cab service to track all their vehicles. 

Working of HTM:
Input temporal data generated from various data sources is semantically encoded as a  sparse array called as sparse distributed representation (SDR).  
This encoded array goes through a processing called spatial pooling to normalize/standardize the  input data from various sources into a sparse output vector or mini-columns (column of pyramidal neurons) of definitive size and fixed sparsity. 
The learning of this spatial pooling is done through Hebbian learning with the boosting of prolonged inactive cells. 
The spatial pooling retains the context of the input data by an algorithm called temporal memory.
initially tries to learn a route that is shown in all red. Gradually it learns each latitude, longitude and velocity with a particular sequence. 
Any change in route or speed or direction will be tracked as an anomaly and reported back.

A: SDR - Sparse Distributed Representation
SDR is simply an array of 0’s and 1’s. If you take a snapshot of neurons in the brain, it is highly likely that you will only see less than 2% neurons in an active state. 
SDR is a mathematical representation of these sparse signals which will likely have less than 2% ones. 
Properties of SDR:
a. SDR is extremely noise resistant. If you introduce noise in magnitude of say 33%, highly likely that the SDR will only match with very like objects. 
You will realize this even for human memory. Try to remember a  name of a professor from your graduation college and try remembering his/her face. It is possible that you might not remember him/her face clearly. 
Even then, if I show you 100 random picture which include your professor’s image, you will definitely locate your professor with high accuracy.
b. SDR can be sub-sampled without loosing much information. Say, your SDR has 20 ones and 10000 zeros. Even if you remove 5 zeros from this SDR, we can still use the sub-sampled SDR to match with new SDRs. You will realize this even for human memory. 
Even if we forget a lot of properties of a fruit which we tasted 10 years back, we can still classify this fruit accurately based on all other properties that we do remember.

B: Semantic Encoding -
We use an encoding engine to take input from an input source and create an SDR. We need to make sure that the encoding algorithm gives us similar SDR for similar objects. 
This concept is very similar to embedding in the deep learning space. 
A lot of pre-built encoders are already available online that include numeric encoding, datetime encoding, English word encoding, 
We need to create your own encoder for specific problems. 
Step 1 – We choose a set of documents that we will use to find semantics of words
Step 2 – We will clean the documents and slice each document into snippets. We will then cluster these snippets so that similar snippets are kept together
Step 3 –  We will now represent each snippet as a node in a SDR
Step 4 – Now we pick up individual (target) words and activate all the nodes (documents) in the SDR that contain our target word. This will create a word fingerprint in form of SDR
Step 5 – We will repeat the above four steps to get word fingerprints for all the words we are interested.
